{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "#选取需要的数据（特定SMART号，特定机型）\n",
    "def data_selection(filespath, features, model):\n",
    "    \n",
    "    #导入文件夹里所有csv文件,按日期排序\n",
    "    all_files = sorted(glob.glob(filespath + \"/*.csv\"))\n",
    "    li = []\n",
    "    columns = []\n",
    "\n",
    "    #选取有用的几列\n",
    "    for f in features:\n",
    "        columns += [\"smart_{}_raw\".format(f)]\n",
    "    columns = [\"date\", 'serial_number', \"failure\"] + columns\n",
    "    \n",
    "    #拼接在一起，每个文件的第一行作为title\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        df = df[df.model == model]\n",
    "        df = df[columns]\n",
    "        li.append(df)\n",
    "\n",
    "    #根据日期排序, 重设index\n",
    "    frame = pd.concat(li, axis=0, ignore_index=True).reset_index(drop=True)\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#平衡数据\n",
    "def data_clean(n_day, frame):\n",
    "    \n",
    "    #找出故障发生当天的机器\n",
    "    fail1 = frame[frame['failure'] == 1]\n",
    "\n",
    "    #将机子出故障前n天的failure都变成1\n",
    "    for s_num in np.array(fail1['serial_number']):\n",
    "        r = n_day\n",
    "        for row in frame.itertuples(): #最快的遍历办法\n",
    "            if frame.at[row.Index,'serial_number'] == s_num: \n",
    "                frame.at[row.Index,'failure'] = 1\n",
    "                r -= 1 \n",
    "            if r < 0: \n",
    "                break\n",
    "\n",
    "    fail = frame[frame['failure'] == 1]\n",
    "    #随机选择使正常运行的案例和失败的一样多\n",
    "    succ = frame[frame['failure'] == 0].sample(n=len(fail.index))\n",
    "    result = pd.concat([fail, succ])\n",
    "    result = result.sort_values(['date'], ignore_index=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#操作的文件夹（在2016年第一个季度上训练，优化模型）\n",
    "filespath = r'/Users/penghanqiu/Desktop/data_Q1_2016'\n",
    "#特定SMART号\n",
    "features = [5, 187, 188, 197, 198]\n",
    "#特定硬盘型号\n",
    "model = \"ST4000DM000\"\n",
    "#向前检查一周，试过3天，%95.8->%99\n",
    "pre_day = 7\n",
    "\n",
    "#选取需要的数据\n",
    "dframe = data_selection(filespath, features, model)[::-1]\n",
    "\n",
    "dframe = data_clean(pre_day, dframe)\n",
    "\n",
    "#获得平衡数据集(耗时20min)\n",
    "dframe.to_csv('clean_data1.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在新的平衡数据集上训练测试\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "filepath = r'/Users/penghanqiu/Desktop/clean_data1.csv'\n",
    "\n",
    "df = pd.read_csv (filepath)\n",
    "df = df.fillna(0)\n",
    "features = [5, 187, 188, 197, 198]\n",
    "columns = []\n",
    "for f in features:\n",
    "    columns += [\"smart_{}_raw\".format(f)]\n",
    "X = df[columns].values\n",
    "y = df['failure'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print出易懂的混淆矩阵:\n",
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    # Print header\n",
    "    print(\"    \" + empty_cell, end=\" \")\n",
    "    print('PREDICTED:')\n",
    "    print(\"     \" + empty_cell, end=\" \")\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n",
    "    print()\n",
    "    # Print rows\n",
    "    print('ACTUAL: ')\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=\" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平衡训练集上准确率: 0.7651483493522775\n",
      "平衡测试集上准确率: 0.7612687813021702\n",
      "\n",
      "\n",
      "            PREDICTED:\n",
      "              Failed Running \n",
      "ACTUAL: \n",
      "     Failed   156.0   143.0 \n",
      "    Running     0.0   300.0 \n"
     ]
    }
   ],
   "source": [
    "# GBDT 单次检验\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "my_classifier = GradientBoostingClassifier(learning_rate=0.06)\n",
    " \n",
    "X_train, X_test, y_train, y_test = tts(X, y, test_size=0.2, random_state = 98)\n",
    "\n",
    "#用训练集的数据标准化\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "#测试集也用训练集得到的参数标准化（无偏）\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "my_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = my_classifier.predict(X_test_scaled)\n",
    "\n",
    "print(\"平衡训练集上准确率:\", my_classifier.score(X_train_scaled, y_train))\n",
    "print(\"平衡测试集上准确率:\", my_classifier.score(X_test_scaled, y_test))\n",
    "print(\"\\n\")\n",
    "\n",
    "#第一行第一列是TP，第一行第二列是FN, 第二行第一列是FP\n",
    "confMat = confusion_matrix(y_test, y_pred,labels=[1,0]) \n",
    "\n",
    "#混淆矩阵\n",
    "print_cm(confMat, [\"Failed\", \"Running\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理后的平衡数据集里:\n",
      "Precision: 0.9643695291986862 \n",
      "recall: 0.5380592323399906 \n",
      "Test Accuracy: 0.7564273789649415 \n",
      "Training Accuracy: 0.7659841203510237 \n",
      "F1 Score: 0.6901928247456602\n"
     ]
    }
   ],
   "source": [
    "# k-fold交叉验证: 选择最优模型 (GBDT)\n",
    "\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "#from sklearn.experimental import enable_hist_gradient_boosting\n",
    "#from sklearn.ensemble import HistGradientBoostingClassifier #基于LightGBM\n",
    "\n",
    "def train_test_classifier(my_classifier, k_fold):\n",
    "    precision, recall, test_acc, train_acc, f1 = ([] for _ in range(5))\n",
    "    for k in range(1, k_fold+1):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = tts(X, y, test_size = 0.2, random_state = k * 10)\n",
    "\n",
    "        #用训练集的数据标准化\n",
    "        scaler = RobustScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "        #测试集也用训练集得到的参数标准化（无偏）\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        my_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "        y_pred_test = my_classifier.predict(X_test_scaled)\n",
    "        y_pred_train = my_classifier.predict(X_train_scaled)\n",
    "        \n",
    "        precision += [precision_score(y_test, y_pred_test)] #TP/(TP + FP)\n",
    "        recall += [recall_score(y_test, y_pred_test)] #TP/(TP + FN)\n",
    "        test_acc += [accuracy_score(y_test, y_pred_test)] #(TP+TN)/(TP+TN+FP+ FN)\n",
    "        train_acc += [accuracy_score(y_train, y_pred_train)] \n",
    "        f1 += [f1_score(y_test, y_pred_test)] #2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return sum(precision)/k_fold, sum(recall)/k_fold, sum(test_acc)/k_fold, sum(train_acc)/k_fold, sum(f1)/k_fold\n",
    "\n",
    "#my_classifier2 = GaussianNB()\n",
    "#my_classifier3 = RandomForestClassifier()\n",
    "my_classifier1 = GradientBoostingClassifier(learning_rate=0.06)\n",
    "#my_classifier4 = HistGradientBoostingClassifier()\n",
    "k_fold = 10\n",
    "\n",
    "precision, recall, test_acc, train_acc, f1 = [0 for _ in range(5)]\n",
    "\n",
    "precision, recall, test_acc, train_acc, f1 = train_test_classifier(my_classifier1, k_fold)\n",
    "\n",
    "print(\"处理后的平衡数据集里:\")\n",
    "print(\"Precision:\", precision, \"\\n\" \"recall:\", recall, \"\\n\" \"Test Accuracy:\",\n",
    "      test_acc,\"\\n\" \"Training Accuracy:\", train_acc,\"\\n\" \"F1 Score:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline检测方法: When the RAW value for one of these five attributes is greater than zero, we predict a hard drive will fail.\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "def baseline_classifier(X,y):\n",
    "    y_pred_base = []\n",
    "\n",
    "    for arr in X:\n",
    "        if all(i == 0 for i in arr):\n",
    "            y_pred_base += [0]\n",
    "        else:\n",
    "            y_pred_base += [1]\n",
    "\n",
    "    precision = precision_score(y, y_pred_base) #TP/(TP + FP)\n",
    "    recall = recall_score(y, y_pred_base) #TP/(TP + FN)\n",
    "    acc = accuracy_score(y, y_pred_base) #(TP+TN)/(TP+TN+FP+ FN)\n",
    "    f1 = f1_score(y, y_pred_base) #2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    #第一行第一列是TP，第一行第二列是FN, 第二行第一列是FP\n",
    "    confMat = confusion_matrix(y, y_pred_base,labels=[1,0]) \n",
    "    \n",
    "    return precision, recall, acc, f1, confMat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在2016年第二个季度上测试模型\n",
    "filespath1 = r'/Users/penghanqiu/Desktop/data_Q2_2016'\n",
    "#特定SMART号\n",
    "features = [5, 187, 188, 197, 198]\n",
    "#特定硬盘型号\n",
    "model = \"ST4000DM000\"\n",
    "\n",
    "#选取需要的数据\n",
    "dframe = data_selection(filespath1, features, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据集里的Baseline查准率: 0.0009105323474465848\n",
      "原始数据集里的Baseline召回率: 0.7631578947368421\n",
      "原始数据集里的Baseline准确率: 0.9389188290192892\n",
      "原始数据集里的Baseline F1值: 0.0018188945511564095\n",
      "\n",
      "\n",
      "            PREDICTED:\n",
      "              Failed Running \n",
      "ACTUAL: \n",
      "     Failed   174.0    54.0 \n",
      "    Running 190923.0 2935459.0 \n"
     ]
    }
   ],
   "source": [
    "df = dframe.fillna(0)\n",
    "features = [5, 187, 188, 197, 198]\n",
    "columns = []\n",
    "for f in features:\n",
    "    columns += [\"smart_{}_raw\".format(f)]\n",
    "X_raw = df[columns].values\n",
    "y_raw = df['failure'].values\n",
    "\n",
    "preci_base, recall_base, acc_base, f1_base, confMat_base = baseline_classifier(X_raw,y_raw)\n",
    "print(\"原始数据集里的Baseline查准率:\", preci_base)\n",
    "print(\"原始数据集里的Baseline召回率:\", recall_base)\n",
    "print(\"原始数据集里的Baseline准确率:\", acc_base)\n",
    "print(\"原始数据集里的Baseline F1值:\", f1_base)\n",
    "print(\"\\n\")\n",
    "#混淆矩阵\n",
    "print_cm(confMat_base, [\"Failed\", \"Running\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            PREDICTED:\n",
      "              Failed Running \n",
      "ACTUAL: \n",
      "     Failed   166.0    62.0 \n",
      "    Running 31540.0 3094842.0 \n",
      "\n",
      "\n",
      "原始数据集里的GBDT的查准率: 0.005235602094240838\n",
      "原始数据集里的GBDT的召回率: 0.7280701754385965\n",
      "模型在原始测试数据集里的准确率: 0.9898925673493016\n",
      "原始数据集里的GBDT的F1值 0.010396442662992424\n"
     ]
    }
   ],
   "source": [
    "#用GBDT单次检验时获得的模型\n",
    "\n",
    "#RobustScaler()标准化，减少outlier的影响\n",
    "X_scaled = scaler.transform(X_raw)\n",
    "\n",
    "y_pred_raw = my_classifier.predict(X_scaled)\n",
    "\n",
    "#第一行第一列是TP，第一行第二列是FN, 第二行第一列是FP\n",
    "confMat_raw1 = confusion_matrix(y_raw, y_pred_raw,labels=[1, 0])\n",
    "#混淆矩阵\n",
    "print_cm(confMat_raw1, [\"Failed\", \"Running\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "precision1 = confMat_raw1[0][0]/(confMat_raw1[0][0]+confMat_raw1[1][0])\n",
    "recall1 = confMat_raw1[0][0]/(confMat_raw1[0][0]+confMat_raw1[0][1])\n",
    "f_1 = 2 * (precision1*recall1)/(precision1+recall1)\n",
    "\n",
    "print(\"原始数据集里的GBDT的查准率:\", precision1) #TP/(TP + FP)\n",
    "print(\"原始数据集里的GBDT的召回率:\", recall1) #TP/(TP + FN)\n",
    "print(\"模型在原始测试数据集里的准确率:\", my_classifier.score(X_scaled, y_raw)) \n",
    "print(\"原始数据集里的GBDT的F1值\", f_1)#2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
